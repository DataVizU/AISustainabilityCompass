export const ModelInfo = [
  {
    model: "falcon",
    size: 40,
    average: 61.48,
    arc: 61.95,
    hellaswag: 85.28,
    mmlu: 56.98,
    truthfulqa: 41.72,
    memory: 33632,
    throughput: 2,
    energy: 18181,
  },
  {
    model: "llama-2",
    size: 70,
    average: 67.35,
    arc: 67.32,
    hellaswag: 87.33,
    mmlu: 69.83,
    truthfulqa: 44.92,
    memory: 53852,
    throughput: 15,
    energy: 130000,
  },
  {
    model: "llama-2",
    size: 26,
    average: 57.29,
    arc: 55.03,
    hellaswag: 79.9,
    mmlu: 53.73,
    truthfulqa: 40.48,
    memory: 31299,
    throughput: 20,
    energy: 200000,
  },
  {
    model: "llama-2",
    size: 13,
    average: 58.66,
    arc: 59.39,
    hellaswag: 82.13,
    mmlu: 55.77,
    truthfulqa: 37.38,
    memory: 28881,
    throughput: 30,
    energy: 332225,
  },
  {
    model: "llama-2",
    size: 7,
    average: 54.32,
    arc: 53.07,
    hellaswag: 78.59,
    mmlu: 46.87,
    truthfulqa: 38.76,
    memory: 22000,
    throughput: 55,
    energy: 500000,
  },
  {
    model: "llama-2",
    size: 3,
    average: 31.88,
    arc: 26.96,
    hellaswag: 26.52,
    mmlu: 23.33,
    truthfulqa: 50.71,
    memory: 9020,
    throughput: 60,
    energy: 670000,
  },
  {
    model: "llama",
    size: 65,
    average: 64.23,
    arc: 63.48,
    hellaswag: 86.09,
    mmlu: 63.93,
    truthfulqa: 43.43,
    memory: 40901,
    throughput: 13,
    energy: 121359,
  },
  {
    model: "llama",
    size: 30,
    average: 61.68,
    arc: 61.26,
    hellaswag: 84.73,
    mmlu: 58.47,
    truthfulqa: 42.27,
    memory: 32556,
    throughput: 19,
    energy: 188679,
  },
  {
    model: "llama",
    size: 13,
    average: 56.08,
    arc: 56.23,
    hellaswag: 80.93,
    mmlu: 47.67,
    truthfulqa: 39.48,
    memory: 14827,
    throughput: 27,
    energy: 332225,
  },
  {
    model: "mpt",
    size: 30,
    average: 56.15,
    arc: 55.89,
    hellaswag: 82.41,
    mmlu: 47.93,
    truthfulqa: 38.38,
    memory: 21997,
    throughput: 28,
    energy: 239808,
  },
  {
    model: "mpt",
    size: 7,
    average: 47.38,
    arc: 47.7,
    hellaswag: 77.57,
    mmlu: 30.8,
    truthfulqa: 33.44,
    memory: 16480,
    throughput: 54,
    energy: 505050,
  },
  {
    model: "stablelm",
    size: 7,
    average: 51.5,
    arc: 47.35,
    hellaswag: 77.08,
    mmlu: 45.1,
    truthfulqa: 36.46,
    memory: 17989,
    throughput: 42,
    energy: 408163,
  },
  {
    model: "stablelm",
    size: 3,
    average: 51.24,
    arc: 46.59,
    hellaswag: 75.94,
    mmlu: 45.23,
    truthfulqa: 37.2,
    memory: 14994,
    throughput: 45,
    energy: 671140,
  },
  {
    model: "gpt-j",
    size: 6,
    average: 42.92,
    arc: 41.38,
    hellaswag: 67.54,
    mmlu: 26.78,
    truthfulqa: 35.96,
    memory: 19298,
    throughput: 23,
    energy: 294117,
  },
  {
    model: "opt",
    size: 13,
    average: 42.53,
    arc: 39.93,
    hellaswag: 71.2,
    mmlu: 24.9,
    truthfulqa: 34.1,
    memory: 14319,
    throughput: 35,
    energy: 346020,
  },
  {
    model: "opt",
    size: 6.7,
    average: 41.88,
    arc: 39.16,
    hellaswag: 68.66,
    mmlu: 24.57,
    truthfulqa: 35.12,
    memory: 8382,
    throughput: 56,
    energy: 653594,
  },
  {
    model: "opt",
    size: 2.7,
    average: 39.56,
    arc: 33.96,
    hellaswag: 61.43,
    mmlu: 25.43,
    truthfulqa: 37.43,
    memory: 5207,
    throughput: 55,
    energy: 704225,
  },
  {
    model: "bloom",
    size: 7,
    average: 42.07,
    arc: 41.13,
    hellaswag: 62,
    mmlu: 26.25,
    truthfulqa: 38.9,
    memory: 9582,
    throughput: 54,
    energy: 606060,
  },
  {
    model: "bloom",
    size: 3,
    average: 39.32,
    arc: 35.75,
    hellaswag: 54.37,
    mmlu: 26.59,
    truthfulqa: 40.57,
    memory: 6103,
    throughput: 56,
    energy: 671140,
  },
  {
    model: "baichuan",
    size: 7,
    average: 47.38,
    arc: 40.7,
    hellaswag: 69.02,
    mmlu: 43.59,
    truthfulqa: 36.23,
    memory: 18270,
    throughput: 34,
    energy: 361010,
  },
  {
    model: "gpt-neox",
    size: 7,
    average: 36.34,
    arc: 28.58,
    hellaswag: 43.94,
    mmlu: 25.38,
    truthfulqa: 47.48,
    memory: 11398,
    throughput: 26,
    energy: 358422,
  },
  {
    model: "xglm",
    size: 7.5,
    average: 39.84,
    arc: 34.13,
    hellaswag: 60.77,
    mmlu: 27.79,
    truthfulqa: 36.66,
    memory: 8370,
    throughput: 56,
    energy: 653594,
  },
  {
    model: "xglm",
    size: 4.5,
    average: 37.68,
    arc: 31.48,
    hellaswag: 57.95,
    mmlu: 25.43,
    truthfulqa: 35.84,
    memory: 8915,
    throughput: 54,
    energy: 628930,
  },
];

export const wordToTokens = (wordNum: number) => wordNum * 2.5;

export const getModel = (modelName: string, modelSize: number) =>
  ModelInfo.filter(
    (model) => model.model === modelName && model.size === modelSize,
  )[0];

export const getModelNum = (
  userNum: number,
  wordNum: number,
  modelName: string,
  modelSize: number,
) => {
  const model = getModel(modelName, modelSize);
  const tokenNum = wordToTokens(wordNum);
  return (userNum * tokenNum) / model.throughput / 8 / 60 / 60;
};

export const getModelEnergy = (
  userNum: number,
  wordNum: number,
  modelName: string,
  modelSize: number,
) => {
  const model = getModel(modelName, modelSize);
  const tokenNum = wordToTokens(wordNum);
  return (userNum * tokenNum) / model.energy;
};

export const getModelWater = (
  userNum: number,
  wordNum: number,
  modelName: string,
  modelSize: number,
) => {
  const energy = getModelEnergy(userNum, wordNum, modelName, modelSize);
  return energy * 2.5;
};

export const getModelK = (
  userNum: number,
  wordNum: number,
  modelName: string,
  modelSize: number,
  cityAlpha: number,
) => {
  const model = getModel(modelName, modelSize);
  const modelNum = getModelNum(userNum, wordNum, modelName, modelSize);
  return modelNum * model.average * cityAlpha;
};
